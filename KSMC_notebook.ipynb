{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## KSMC Project\n",
    "Jonathan Armitage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set presented is from a Kaggle competition, [link](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management). The company providing the data is BNP Paribas Cardif, an insureer looking to use quantitative methods to improve the claims process as it relates to servicing their customer. \n",
    "\n",
    "The systems in place that were used for this project:\n",
    "\n",
    "* Python distrubution through Continuum Analytics (Anaconda)\n",
    "* Apache Spark\n",
    "    + PySpark with Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "This data set was in need of a great deal of preprocessing due the missing data, and the absolute number of categories that were found. However this was dealt with by simplyfing the process into two seperate data cleaning procedures, clean categorical features and then clean numeric features. Partitioning it this way allowed for more concise, simplified code since some numerical methods are apt of cleaning categorical data (ex. counting values) while others are better for numerical data.\n",
    "\n",
    "Prediction of the test set was done by way of a logisitic regression utilizing an 'l1' penalty to envoke sparsity. Utilizing th 'l1' penalty serves as a feature selection mechanism given that it shrinks some (noninformative) coefficients to approximately zero. Per the Kaggle competition website particpants classification models were judged on the basis of log loss or cross entropy. Therefore, I scored my model using the performance as well.\n",
    "\n",
    "### Ideas I Would Have Liked to Try:\n",
    "1. Ability to create / find a better distrubution of the categorical features\n",
    "2. Utilize the pipeline feature to tune hyperparameters of ensemble models, as well as the logisitc regreesion I used. 3. Better use of cross validation (for generalization error, and interpolation methods)\n",
    "4. Include more visualizations of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules to enable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import log_loss, auc, classification_report, confusion_matrix, roc_auc_score, f1_score, label_ranking_loss\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.learning_curve import validation_curve, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from time import process_time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'svg',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import raw data\n",
    "trainBNP = pd.read_csv('train.csv')\n",
    "testBNP = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lots of of missing\n",
    "    # pctNan = percentage of all elements that have NaN\n",
    "\n",
    "tElements_train = np.sum( trainBNP.count(numeric_only = False).values )\n",
    "tElements_Numeric_train = np.sum( trainBNP.count(numeric_only = True).values )\n",
    "tElements_NaN_train = tElements_train - tElements_Numeric_train\n",
    "pctNaN = tElements_NaN_train / tElements_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements Missing (percentage): 0.202\n"
     ]
    }
   ],
   "source": [
    "print('Elements Missing (percentage): %.3f' % (pctNaN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that a large part of the data set is missing, approximately 20%, this function will be used to cerate a dataframe of only the categorical features. This will ease the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def catVar_func(df_train):\n",
    "    \n",
    "    \"\"\"df_train = trainBNP\"\"\"\n",
    "    \"\"\"create list of categorical features -- map them to dataframe\"\"\"\n",
    "\n",
    "    catVar_train = []\n",
    "    for k in df_train.columns:\n",
    "        if df_train[k].dtype == 'O':\n",
    "            catVar_train.append(k)\n",
    "\n",
    "    train_catVar = pd.DataFrame(data = df_train, columns = catVar_train)  \n",
    "    \n",
    "    \"\"\"imputing missing values to most frequent category -- this allows for a one-to-one transformation that \n",
    "    does not cause issues when predicting onto the test set\"\"\"\n",
    "    \n",
    "    for y in train_catVar.columns:\n",
    "        train_catVar[y].fillna(value = train_catVar[y].value_counts().idxmax(), inplace = True)\n",
    "    \n",
    "    train_catVar_wTgt = pd.concat([df_train['target'], train_catVar], axis = 1)\n",
    "    \n",
    "    return train_catVar, train_catVar_wTgt, catVar_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_catVar, train_catVar_wTgt, catVar_train = catVar_func(df_train=trainBNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used another function for categorical features in order to parse them relative a threshold stated in terms of the number of categories with in each categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def catGroups(df_train_cat, catVar_train, thresH, minFac, maxFac):\n",
    "    \n",
    "    \"\"\"df_cat = trainBNP_catvar_wTgt\"\"\"\n",
    "    \n",
    "    \"\"\"this function is meant to parse the categorical features by first grouping \n",
    "    them together, calculating the number of categories within each feature and \n",
    "    benchmark each feature against the target: the absolute number of categories, \n",
    "    and the condtional expectation E[Tgt | catVar_i], benchmarked against the unconditional \n",
    "    expectation of the outcome, E[Tgt]\"\"\"\n",
    "   \n",
    "    meanTgt_train = []\n",
    "    levl_train = []\n",
    "\n",
    "    for t in catVar_train:\n",
    "        meanTgt_train.append(np.mean(df_train_cat.groupby(t)['target'].aggregate(np.mean)))\n",
    "        levl_train.append(len(df_train_cat[t].value_counts()))\n",
    "    \n",
    "    \"\"\"new dataframe constructed to house the categorical features\"\"\"\n",
    "    \n",
    "    dfCatVar = pd.DataFrame(columns = ['catTgt_Mean', 'numbCat'], index = catVar_train)\n",
    "    dfCatVar['catTgt_Mean'] = meanTgt_train\n",
    "    dfCatVar['numbCat'] = levl_train\n",
    "    dfCatVar['baseFactor'] = dfCatVar['catTgt_Mean'] / df_train_cat['target'].mean()\n",
    "    \n",
    "    \"\"\"create binary factors based on researcher based thresholds (number of categories [catThr],\n",
    "    and relative increase/decrease over the univariate target mean [minFac <= benchmark >= maxFac])\"\"\"\n",
    "\n",
    "    catThr = thresH\n",
    "    dfCatVar['inclCat_c1'] = np.where((dfCatVar['numbCat'] <= catThr), 1, 0)\n",
    "    dfCatVar['inclCat_c2'] = np.where((dfCatVar['baseFactor'] <= minFac) | (dfCatVar['baseFactor'] >= maxFac), 1, 0)\n",
    "    dfCatVar['incl_ovl'] = dfCatVar['inclCat_c1'] + dfCatVar['inclCat_c2']\n",
    "    \n",
    "    \"\"\"grabs the features that met both the category threshold and creates a dataframe of dummy variables \"\"\"\n",
    "    \n",
    "    catVar_nextStep = []\n",
    "    for z in dfCatVar.index:\n",
    "        if dfCatVar.loc[z, 'incl_ovl'] > 1:\n",
    "            catVar_nextStep.append(z)\n",
    "    \n",
    "    dfTrain_catDum = pd.get_dummies(df_train_cat[catVar_nextStep])\n",
    "    \n",
    "    return dfTrain_catDum, catVar_nextStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain_catDum, catVar_nextStep = catGroups(df_train_cat=train_catVar_wTgt, catVar_train=catVar_train,\n",
    "                                            thresH=200, minFac=0.99, maxFac=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, categorical features were split off from numeric features into their own dataframe. The same thing is occuring within this function except there is an interpolation method ('intMethod') specified as a parameter of the function. Interpolation was necessary asbecause of the vast amounts of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numVar_func(df_train, intMethod):\n",
    "\n",
    "    \"\"\"df = trainBNP\"\"\"\n",
    "    \"\"\"intMethod = 'linear', 'quadratic', 'nearest' \"\"\"\n",
    "    \"\"\"generates columns that are not categorical and appends them to a list. Those columns are then placed into\n",
    "    a dataframe where an interpolation mechanism is administered\"\"\"\n",
    "    \n",
    "    # create list of numerical features\n",
    "\n",
    "    numVar = []\n",
    "    for a in df_train.columns[2:len(df_train.columns)]:\n",
    "        if df_train[a].dtype != 'O':\n",
    "            numVar.append(a)\n",
    "        \n",
    "    trainBNP_numVar = pd.DataFrame(data = df_train, columns = numVar)\n",
    "\n",
    "    dfTrain_numVar = trainBNP_numVar.interpolate(method = intMethod)\n",
    "    \n",
    "    return dfTrain_numVar, numVar, intMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain_numVar, numVar, intMethod = numVar_func(df_train = trainBNP, intMethod='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction purposes onto the test set, the test set needs to mirror the training set in terms of the categorical variables selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testSet(df_test, numVar, catVar_nextStep, intMethod = 'intMethod'):\n",
    "    \n",
    "    test_catVar = pd.DataFrame(data = df_test, columns = catVar_nextStep) \n",
    "    \n",
    "    for n in test_catVar.columns:\n",
    "        test_catVar[n].fillna(value = test_catVar[n].value_counts().idxmax(), inplace = True)\n",
    "    \n",
    "    #test_catVar.fillna(value = 'NA', inplace = True)\n",
    "    dfTest_catDum = pd.get_dummies(test_catVar[catVar_nextStep])\n",
    "    \n",
    "    test_numVar = pd.DataFrame(data = df_test, columns = numVar)\n",
    "    dfTest_numVar = test_numVar.interpolate(method = intMethod)\n",
    "    \n",
    "    X_test = np.concatenate((dfTest_catDum, dfTest_numVar), axis = 1)\n",
    "    \n",
    "    return X_test, dfTest_catDum, dfTest_numVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, dfTest_catDum, dfTest_numVar = testSet(df_test=testBNP, numVar=numVar,\n",
    "                                               catVar_nextStep=catVar_nextStep, intMethod=intMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to diffenences in the categorical variables in the train set and test set, i.e. a categorical feature \n",
    "in one set may have a disparate number of categories relative to the other set, the dimensionality of the sets need to be ammendded to equal each other. Thus, the dimensionality of the set with most categorical features after dummy varible transformation must be shrunk to equal the dimensionality of the set with the least categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dimChange(df_train_cat, df_test_cat):\n",
    "    \n",
    "    \"\"\"df_train_cat = dfTrain_catDum, df_test_cat = dfTest_catDum\"\"\"\n",
    "    \n",
    "    \"\"\"due to diffenences in the categorical variables in the train set and test set, i.e. a categorical feature\n",
    "    in one set may have a disparate number of categories relative to the other set. Thus, the dimensionality of\n",
    "    the set with most categorical features after dummy varible transformation must be shrunk to equal the\n",
    "    dimensionality of the set with the least categorical features.\"\"\"\n",
    "    \n",
    "    if df_train_cat.shape[1] > df_test_cat.shape[1]:\n",
    "        dfTrain_catDum2 = pd.DataFrame(df_train_cat, columns = df_test_cat.columns)\n",
    "        dfTrain_catDum2.fillna(value = 'NA', inplace = True)\n",
    "        dfTrain_catDum3 = pd.get_dummies(dfTrain_catDum2[df_test_cat.columns])                      \n",
    "    \n",
    "    return dfTrain_catDum3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain_catDum3 = dimChange(df_train_cat = dfTrain_catDum, df_test_cat = dfTest_catDum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the training set occurs within this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainSet(df_train, df_train_cat, df_train_numVar, test_size = 0.20):\n",
    "    \n",
    "    \"\"\"df = trainBNP, df_train_cat = dfTrain_catDum3, df_train_nnumVar = dfTrain_numVar, test_size defaults to 0.20 \"\"\"\n",
    "    \"\"\"function to prepare data for models\"\"\"\n",
    "\n",
    "    X = np.concatenate((df_train_cat, df_train_numVar), axis = 1)\n",
    "    y = np.asarray(df_train['target'])\n",
    "    \n",
    "    X_train, X_test_fT, y_train, y_test_fT = train_test_split(X, y, test_size = test_size, random_state = 1)\n",
    "    \n",
    "    return X_train, y_train, X_test_fT, y_test_fT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test_fT, y_test_fT = trainSet(df_train=trainBNP, df_train_cat=dfTrain_catDum3, \n",
    "                                                  df_train_numVar=dfTrain_numVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression used to model the respone variable with a 'l1' regulariztion parameter ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR = LogisticRegression(penalty='l1')\n",
    "clfLR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions made on sets derived from trainSet function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probPred_train = clfLR.predict_proba(X_test_fT)\n",
    "probPred_train2 = clfLR.predict_proba(X_train)\n",
    "probPred_test = clfLR.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Loss calculated for sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ll_test_fT = log_loss(y_true = y_test_fT, y_pred = probPred_train) \n",
    "ll_trainSet = log_loss(y_true = y_train, y_pred = probPred_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss Train Set: 0.485\n",
      "Log Loss held Out Set: 0.481\n"
     ]
    }
   ],
   "source": [
    "print('Log Loss Train Set: %.3f' % (ll_trainSet))\n",
    "print('Log Loss held Out Set: %.3f' % (ll_test_fT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is where I was going to begin using the pipeline module in sklearn to tune different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "clf = LogisticRegression(penalty='l1')\n",
    "# scale the matrix X to have 0 mean and unit variance, then perform PCA, \n",
    "# and then fit a logistic regression model\n",
    "n_comps = np.round(np.arange(1, 50, 10))\n",
    "Cst = [0.001, 0.1, 1.0, 10.0]\n",
    "#plts = ['l1', 'l2']\n",
    "\n",
    "#params = dict(pca__n_components = n_comps,\n",
    "#            clf__C = Cst,\n",
    "#          clf__penalty = plts)\n",
    "\n",
    "params = dict(clf__C = Cst)\n",
    "             \n",
    "             \n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', clf)])\n",
    "cv_count = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est_m1 = GridSearchCV(pipe_lr, param_grid = params, cv = cv_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_m1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_m1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_m1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_m1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_m1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
